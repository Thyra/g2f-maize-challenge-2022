{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767d7161-93bf-4fb2-b3d2-b8cf559e49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "functions_at = '/proj/modeling/abhi'\n",
    "sys.path = [f'{functions_at}'] + sys.path\n",
    "from model_CNN_source import *\n",
    "tmp_at = '/proj/tmp_data'\n",
    "dump_at = '/proj/dump'\n",
    "source_data = '/proj/source_data/Training_Data'\n",
    "source_data_2 = '/proj/source_data/Testing_Data'\n",
    "processed_data = '/proj/processed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "023eab48-aa25-4639-83ca-a95212f67a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## trait_data\n",
    "trait_data = pd.read_csv(f\"{source_data}/1_Training_Trait_Data_2014_2021.csv\")\n",
    "trait_data = trait_data[trait_data.Yield_Mg_ha.notnull()]\n",
    "## meta data\n",
    "meta_data = pd.read_csv(f\"{source_data}/2_Training_Meta_Data_2014_2021_utf_encoded.csv\").add_prefix(\"mt_dta_\")\n",
    "## geno data\n",
    "geno_data = pd.read_csv(f\"{processed_data}/geno_processed.miss.1.mac.1.biallelic.txt\").add_prefix(\"ge_dta_\")\n",
    "geno_data = geno_data.iloc[:, 0:10] # it haas no missing data\n",
    "## soil data\n",
    "soil_data = pd.read_csv(f\"{source_data}/3_Training_Soil_Data_2015_2021.csv\").add_prefix(\"sl_dta_\")\n",
    "## weather data\n",
    "weather_data = pd.read_csv(f\"{source_data}/4_Training_Weather_Data_2014_2021.csv\").add_prefix(\"wt_dta_\")\n",
    "## EC data\n",
    "ec_data = pd.read_csv(f\"{source_data}/6_Training_EC_Data_2014_2021.csv\").add_prefix(\"ec_dta_\")\n",
    "# testing data to be provided\n",
    "submission_data = pd.read_csv(f\"{source_data_2}/1_Submission_Template_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ec267f-8225-4ec3-85e7-2e519fc2a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with excess missing data\n",
    "\n",
    "# pheno data \n",
    "pheno_data = purge_excess_missing(trait_data, id_cols = [\"Env\", \"Year\", \"Field_Location\", \"Experiment\", \"Replicate\", \"Block\", \"Plot\"], plot = False) # the plot shows missing values in columns not in the id_cols list\n",
    "\n",
    "# meta data\n",
    "meta_data_purged = purge_excess_missing(meta_data, id_cols = [\"mt_dta_Year\", \"mt_dta_Env\", \"mt_dta_Experiment_Code\"], plot = False)\n",
    "\n",
    "# soil_data overview\n",
    "soil_data_purged = purge_excess_missing(soil_data, id_cols = [\"sl_dta_Year\", \"sl_dta_Env\"], plot = False)\n",
    "\n",
    "# weather data reshape\n",
    "weather_data.groupby([\"wt_dta_Env\"])[\"wt_dta_Env\"].count()\n",
    "weather_data[\"wt_dta_moth_dy\"] = weather_data[\"wt_dta_Date\"].astype(str).str[4:8]\n",
    "weather_data_wide = weather_data.pivot(index=\"wt_dta_Env\", columns=\"wt_dta_moth_dy\", values=weather_data.columns.tolist()[2:-1])\n",
    "weather_data_wide.columns = ['_'.join(map(str, x)) for x in weather_data_wide.columns] #212 * (16*366)\n",
    "weather_data_wide_df = weather_data_wide.reset_index()\n",
    "weather_data_purged = purge_excess_missing(weather_data_wide_df, id_cols = [\"wt_dta_Env\"], plot = False)\n",
    "\n",
    "# ec data\n",
    "ec_data_purged = purge_excess_missing(ec_data, id_cols = [\"ec_dta_Env\"], plot = False)\n",
    "\n",
    "# remove objects to free memory\n",
    "#del trait_data\n",
    "#del meta_data\n",
    "#del soil_data\n",
    "#del weather_data\n",
    "#del weather_data_wide\n",
    "#del ec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fff68f98-bdeb-4432-9946-d771be58b74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pheno_data has 217 environments, purge 1 removes 5, purge 2 removes 52, purge 3 removes 51, thus the final data has 109 environments\n",
      "purge 4 removed 1553 rows due to non availability of genotypic data. Data now has 109 environments and 3827 unique genotypes in 69360 rows\n"
     ]
    }
   ],
   "source": [
    "# remove those environments which are completele missing soil, weather, or ec data\n",
    "purge_1 = pheno_data[pheno_data.Env.isin(weather_data_purged.wt_dta_Env.unique())]\n",
    "purge_2 = purge_1[purge_1.Env.isin(ec_data_purged.ec_dta_Env.unique())]\n",
    "purge_3 = purge_2[purge_2.Env.isin(soil_data_purged.sl_dta_Env.unique())]\n",
    "purge_4 = purge_3[purge_3.Hybrid.isin(geno_data.ge_dta_Hybrid.unique())]\n",
    "\n",
    "print(f'pheno_data has {pheno_data.Env.nunique()} environments, purge 1 removes {pheno_data.Env.nunique() - purge_1.Env.nunique()}, purge 2 removes {purge_1.Env.nunique() - purge_2.Env.nunique()}, purge 3 removes {purge_2.Env.nunique() - purge_3.Env.nunique()}, thus the final data has {purge_3.Env.nunique()} environments')\n",
    "print(f'purge 4 removed {purge_3.shape[0] - purge_4.shape[0]} rows due to non availability of genotypic data. Data now has {purge_4.Env.nunique()} environments and {purge_4.Hybrid.nunique()} unique genotypes in {purge_3.shape[0]} rows')\n",
    "\n",
    "# we can pull env and soil data from public data bases to put them back in but lets start with a conservative set.\n",
    "\n",
    "# join data together for the environments with all data available\n",
    "merged_data = purge_4.merge(soil_data_purged, how=\"left\", left_on=\"Env\", right_on=\"sl_dta_Env\").merge(weather_data_purged, how=\"left\", left_on=\"Env\", right_on=\"wt_dta_Env\").merge(ec_data_purged, how=\"left\", left_on=\"Env\", right_on=\"ec_dta_Env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664d75dd-91f9-4b14-9368-9b84ea52eec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing data freuency for all explanotory variables in each env\n",
    "id_cols = [\"Env\"]\n",
    "data_missing = merged_data[merged_data.columns.difference(id_cols)].isna()\n",
    "data_missing = pd.concat([merged_data.loc[:, id_cols], data_missing], axis = 1)\n",
    "data_missing_overview = data_missing.groupby(id_cols[0]).sum()/merged_data.groupby(\"Env\").count() # gives propotions of missing values per env for a given variable\n",
    "#for i in range(data_missing_overview.shape[0]):\n",
    "#    print(pd.cut(x=data_missing_overview.iloc[i, :].values.tolist(), bins=[-0.1, 0, 0.20, 0.40, 0.60, 0.80, 1]).unique()) # mostly less than 20 percent missing epr env. so i impute these\n",
    "data_missing_overview.to_csv(f\"{processed_data}/missign_overview_per_env.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae37f1e-0ab0-40f1-a92c-5d43b1d7135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define coltypes to impute\n",
    "coltypes = merged_data[merged_data.columns.difference(pheno_data.columns)].dtypes.values.astype(\"str\")\n",
    "data_select = []\n",
    "for Type in coltypes:\n",
    "    if \"float64\" in Type:\n",
    "        data_select.append(True)\n",
    "    elif \"int64\" in Type :\n",
    "        data_select.append(True)\n",
    "    else:\n",
    "        data_select.append(False)\n",
    "data_nonselect = [not x for x in data_select]\n",
    "col_select = merged_data[merged_data.columns.difference(pheno_data.columns)].columns[data_select].tolist()\n",
    "col_select_with_env = [\"Env\"]+col_select \n",
    "col_nonselect = merged_data[merged_data.columns.difference(pheno_data.columns)].columns[data_nonselect].tolist()\n",
    "col_nonselect = pheno_data.columns.tolist() + col_nonselect # will not be imputed\n",
    "\n",
    "merged_data_raw = merged_data.loc[:, col_nonselect]\n",
    "merged_data_to_impute = merged_data.loc[:, col_select_with_env]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e500fc49-1ecc-4b0e-be61-8ff37859d59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished for 0\n",
      "sl_dta_% Clay has issues with median value calculation. check it manually\n",
      "sl_dta_% Sand has issues with median value calculation. check it manually\n",
      "sl_dta_% Silt has issues with median value calculation. check it manually\n",
      "sl_dta_%Ca Sat has issues with median value calculation. check it manually\n",
      "sl_dta_%H Sat has issues with median value calculation. check it manually\n",
      "sl_dta_%K Sat has issues with median value calculation. check it manually\n",
      "sl_dta_%Mg Sat has issues with median value calculation. check it manually\n",
      "sl_dta_%Na Sat has issues with median value calculation. check it manually\n",
      "sl_dta_1:1 S Salts mmho/cm has issues with median value calculation. check it manually\n",
      "sl_dta_1:1 Soil pH has issues with median value calculation. check it manually\n",
      "sl_dta_CEC/Sum of Cations me/100g has issues with median value calculation. check it manually\n",
      "sl_dta_Calcium ppm Ca has issues with median value calculation. check it manually\n",
      "sl_dta_E Depth has issues with median value calculation. check it manually\n",
      "sl_dta_Magnesium ppm Mg has issues with median value calculation. check it manually\n",
      "sl_dta_Nitrate-N ppm N has issues with median value calculation. check it manually\n",
      "sl_dta_Organic Matter LOI % has issues with median value calculation. check it manually\n",
      "sl_dta_Sodium ppm Na has issues with median value calculation. check it manually\n",
      "sl_dta_Sulfate-S ppm S has issues with median value calculation. check it manually\n",
      "sl_dta_Texture No has issues with median value calculation. check it manually\n",
      "sl_dta_WDRF Buffer pH has issues with median value calculation. check it manually\n",
      "sl_dta_lbs N/A has issues with median value calculation. check it manually\n",
      "finished for 1000\n",
      "finished for 2000\n",
      "finished for 3000\n",
      "finished for 4000\n",
      "finished for 5000\n",
      "finished for 6000\n"
     ]
    }
   ],
   "source": [
    "# impute \n",
    "store_exceptions = []\n",
    "merged_data_imputed = merged_data_to_impute.iloc[:, 0:1]\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    for i, col in enumerate(col_select):\n",
    "        if i%1000 == 0:\n",
    "            print(f'finished for {i}')\n",
    "        val = None\n",
    "        val = merged_data_to_impute.groupby(['Env'])[col].apply(lambda x: x.fillna(x.median()))\n",
    "        if val.isnull().sum() == 0:\n",
    "            merged_data_imputed = pd.concat([merged_data_imputed, val], axis = 1) # store only those columns which can be imputed\n",
    "        else:\n",
    "            store_exceptions.append(col)\n",
    "            print(f'{col} has issues with median value calculation. check it manually')\n",
    "# sanity check\n",
    "data_missing = merged_data_imputed.isna().sum()/merged_data_imputed.shape[0]\n",
    "data_missing[data_missing != 0]\n",
    "final_imputed_data = pd.concat([merged_data_raw, merged_data_imputed[merged_data_imputed.columns.difference([\"Env\"])]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73996fd1-92e3-4b7c-8703-2b14a509a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_imputed_data.to_csv(f\"{processed_data}/combined_mat.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "664e31da-0934-499f-b545-e7c8223d4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming that we arrive on object called final_data after filtereing and if needed some imputation\n",
    "\n",
    "final_data = final_imputed_data.loc[:, ['Env', 'Year', \"Hybrid\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2b6f66a-ca39-4425-a074-8e27b64aeb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train test splits and save them V_1: true cv without a val set. you can sample 10 percent from the trian set to be val set\n",
    "\n",
    "fold = 10\n",
    "runs = 100\n",
    "run = 0\n",
    "out_dict_1 = {}\n",
    "while run < runs:\n",
    "    out_dict_2 = {}\n",
    "    if run > 0:\n",
    "        del kf\n",
    "    kf = KFold(n_splits=fold, random_state=40+(20*run), shuffle=True)\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf.split(final_data.index):\n",
    "        in_dict = {}\n",
    "        in_dict[\"train\"] = train_index.tolist() # does not work since the train set looses all info \n",
    "        in_dict[\"test\"] = test_index.tolist()\n",
    "        out_dict_2[fold] = in_dict\n",
    "        fold += 1\n",
    "    out_dict_1[run] = out_dict_2\n",
    "    run += 1\n",
    "# write json file\n",
    "# write_json(data = out_dict_1, path = f\"{processed_data}/train_test_split.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8386e9b1-ce93-4c18-acb0-329301ad9eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# create train test splits and save them V_2. Custom cv with val set\n",
    "\n",
    "sets={}\n",
    "hld_year_geno=[]\n",
    "\n",
    "total_years = final_data.Year.unique()\n",
    "for year in total_years:\n",
    "    for rep in range(0,10):\n",
    "        # test data\n",
    "        test_data = final_data[final_data.Year.isin([year])]\n",
    "        test_geno_total = test_data.Hybrid.unique().tolist()\n",
    "        test_geno_sub = random.sample(test_geno_total, int(len(test_geno_total)*0.2))\n",
    "        test_set = test_data[test_data.Hybrid.isin(test_geno_sub)]\n",
    "        \n",
    "        # val data\n",
    "        train_data = final_data[~final_data.Year.isin([year])]\n",
    "        train_data_sub = train_data[~train_data.Hybrid.isin(test_geno_sub)]\n",
    "        \n",
    "        val_idx = sorted(random.sample(train_data_sub.index.tolist(), int(0.1*len(train_data_sub.index.tolist()))))\n",
    "        val_set = train_data_sub.loc[val_idx].copy()\n",
    "        \n",
    "        # train_data\n",
    "        train_set = train_data_sub[~train_data_sub.index.isin(val_idx)]\n",
    "        \n",
    "        # sanity checks\n",
    "        if len(train_set[train_set[\"Year\"].isin(test_set[\"Year\"].unique().tolist())]) !=0:\n",
    "            print(\"CONTAMINATED SETS: Year\")\n",
    "        if len(train_set[train_set[\"Hybrid\"].isin(test_set[\"Hybrid\"].unique().tolist())]) !=0:\n",
    "            print(\"CONTAMINATED SETS: Genotype\")\n",
    "            \n",
    "        # produce output\n",
    "        sets[str(year)+\"@\"+str(rep)]={\"train\":train_set.index.tolist(),\n",
    "                 \"val\":val_set.index.tolist(),\n",
    "                 \"test\":test_set.index.tolist()}\n",
    "                #record data for diagnostic purposes\n",
    "        hld_year_geno.append([year, rep, len(test_set[\"Hybrid\"].unique())/len(test_geno_total),\n",
    "                              len(train_set[\"Hybrid\"].unique())/len(final_data[\"Hybrid\"].unique()),\n",
    "                              len(train_set), len(val_set), len(test_set)])\n",
    "print(len(sets))\n",
    "# write json file\n",
    "write_json(data = sets, path = f\"{processed_data}/train_test_split_v2.json\") # can think of saving the hld_year_geno also if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bdb7a57-1c35-4b17-a90e-770131cac5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_712/1384639663.py:2: DtypeWarning: Columns (24,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training_data = pd.read_csv(f\"{processed_data}/combined_mat.csv\")\n"
     ]
    }
   ],
   "source": [
    "## submission data\n",
    "training_data = pd.read_csv(f\"{processed_data}/combined_mat.csv\")\n",
    "training_data['type'] = \"train\"\n",
    "\n",
    "## testing data to be provided\n",
    "submission_data = pd.read_csv(f\"{source_data_2}/1_Submission_Template_2022.csv\")\n",
    "submission_data['type'] = \"submission\"\n",
    "\n",
    "## geno data\n",
    "geno_data = pd.read_csv(f\"{processed_data}/geno_processed.miss.1.mac.1.biallelic.txt\").add_prefix(\"ge_dta_\")\n",
    "geno_data = geno_data.iloc[:, 0:10] # it haas no missing data\n",
    "\n",
    "## soil data\n",
    "soil_data_sub = pd.read_csv(f\"{source_data_2}/3_Testing_Soil_Data_2022.csv\").add_prefix(\"sl_dta_\")\n",
    "## weather data\n",
    "weather_data_sub = pd.read_csv(f\"{source_data_2}/4_Testing_Weather_Data_2022.csv\").add_prefix(\"wt_dta_\")\n",
    "## EC data\n",
    "ec_data_sub = pd.read_csv(f\"{source_data_2}/6_Testing_EC_Data_2022.csv\").add_prefix(\"ec_dta_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdb6bcea-4292-4b5e-9d02-cf8b7b757d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## genertate submission_data in the same column format as training data\n",
    "# remove those environments which are completele missing soil, weather, or ec data\n",
    "\n",
    "# soil_data overview\n",
    "soil_data_sub_purged = purge_excess_missing(soil_data_sub, id_cols = [\"sl_dta_Year\", \"sl_dta_Env\"], plot = False)\n",
    "\n",
    "# weather data reshape\n",
    "weather_data_sub.groupby([\"wt_dta_Env\"])[\"wt_dta_Env\"].count() # 314 days instead of 365 days\n",
    "weather_data_sub[\"wt_dta_moth_dy\"] = weather_data_sub[\"wt_dta_Date\"].astype(str).str[4:8]\n",
    "weather_data_sub_wide = weather_data_sub.pivot(index=\"wt_dta_Env\", columns=\"wt_dta_moth_dy\", values=weather_data_sub.columns.tolist()[2:-1])\n",
    "weather_data_sub_wide.columns = ['_'.join(map(str, x)) for x in weather_data_sub_wide.columns] #212 * (16*366)\n",
    "weather_data_sub_wide_df = weather_data_sub_wide.reset_index()\n",
    "weather_data_sub_purged = purge_excess_missing(weather_data_sub_wide_df, id_cols = [\"wt_dta_Env\"], plot = False)\n",
    "\n",
    "# ec data\n",
    "ec_data_sub_purged = purge_excess_missing(ec_data_sub, id_cols = [\"ec_dta_Env\"], plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce2d5401-7e14-4b59-92b0-7bfb74fcc4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove those environments which are completele missing soil, weather, or ec data\n",
    "purge_sub_1 = submission_data[submission_data.Env.isin(weather_data_sub_purged.wt_dta_Env.unique())]\n",
    "purge_sub_2 = purge_sub_1[purge_sub_1.Env.isin(ec_data_sub_purged.ec_dta_Env.unique())]\n",
    "purge_sub_3 = purge_sub_2[purge_sub_2.Env.isin(soil_data_sub_purged.sl_dta_Env.unique())]\n",
    "purge_sub_4 = purge_sub_3[purge_sub_3.Hybrid.isin(geno_data.ge_dta_Hybrid.unique())]\n",
    "\n",
    "purge_sub_4.groupby('Env').count().shape # has data for only 20 out of 26 or so envt.\n",
    "\n",
    "merged_data_sub = purge_sub_4.merge(soil_data_sub_purged, how=\"left\", left_on=\"Env\", right_on=\"sl_dta_Env\").merge(weather_data_sub_purged, how=\"left\", left_on=\"Env\", right_on=\"wt_dta_Env\").merge(ec_data_sub_purged, how=\"left\", left_on=\"Env\", right_on=\"ec_dta_Env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89da66ae-ef43-4792-b3d9-6d04533f4820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variable</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wt_dta_ALLSKY_SFC_PAR_TOT</th>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_ALLSKY_SFC_SW_DNI</th>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_ALLSKY_SFC_SW_DWN</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_GWETPROF</th>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_GWETROOT</th>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_GWETTOP</th>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_PRECTOTCORR</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_PS</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_QV2M</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_RH2M</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_T2M</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_T2MDEW</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_T2MWET</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_T2M_MAX</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_T2M_MIN</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt_dta_WS2M</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           days\n",
       "variable                       \n",
       "wt_dta_ALLSKY_SFC_PAR_TOT   305\n",
       "wt_dta_ALLSKY_SFC_SW_DNI    305\n",
       "wt_dta_ALLSKY_SFC_SW_DWN     62\n",
       "wt_dta_GWETPROF             305\n",
       "wt_dta_GWETROOT             305\n",
       "wt_dta_GWETTOP              305\n",
       "wt_dta_PRECTOTCORR           56\n",
       "wt_dta_PS                    56\n",
       "wt_dta_QV2M                  56\n",
       "wt_dta_RH2M                  56\n",
       "wt_dta_T2M                   56\n",
       "wt_dta_T2MDEW                56\n",
       "wt_dta_T2MWET                56\n",
       "wt_dta_T2M_MAX               56\n",
       "wt_dta_T2M_MIN               56\n",
       "wt_dta_WS2M                  56"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## whats missing\n",
    "miss_cols = {}\n",
    "miss_cols['wc'] = weather_data_purged.columns[~weather_data_purged.columns.isin(weather_data_sub_purged.columns)]\n",
    "miss_cols['ec'] = ec_data_purged.columns[~ec_data_purged.columns.isin(ec_data_sub_purged.columns)]\n",
    "miss_cols['sl'] = soil_data_purged.columns[~soil_data_purged.columns.isin(soil_data_sub_purged.columns)]\n",
    "miss_cols # 2147 missing only for weather data\n",
    "info_missing = pd.DataFrame({\"days\" : [int(f'2022{x[-4:]}') for x in miss_cols['wc']],\n",
    "                            \"variable\" : [x[0:-5] for x in miss_cols['wc']]})\n",
    "vars_miss = info_missing.variable.values.tolist()\n",
    "info_missing.groupby(['variable']).count() # different number of days are missing for different variables\n",
    "\n",
    "# more details\n",
    "#to_check_var = np.unique(vars_miss).tolist()[1]\n",
    "#respective_days = info_missing.loc[info_missing.variable == to_check_var, \"days\"].values.tolist()\n",
    "#weather_data_sub.loc[weather_data_sub.wt_dta_Date.isin(respective_days), to_check_var] # some days are missing and others have completely missing data. I stop here to investigate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58da740c-c25b-4a8d-a71c-5f6165039ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_overview = merged_data_sub.isna().sum()\n",
    "columns_with_na = missing_overview[missing_overview > 0].index.tolist()[1:] # only few columns. i will just remove them except the first one\n",
    "merged_data_sub_no_missing = merged_data_sub.loc[:, ~merged_data_sub.columns.isin(columns_with_na)]\n",
    "training_data_filtered = training_data.loc[:, training_data.columns.isin(merged_data_sub_no_missing.columns)]\n",
    "really_final_data = pd.concat([training_data_filtered, merged_data_sub_no_missing]) # loose a lot of columns which are not there in submission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c95a65f-0144-4d7b-9435-5d742bc0f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "really_final_data.to_csv(f\"{processed_data}/combined_mat_w_o_BLUEs_final.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
