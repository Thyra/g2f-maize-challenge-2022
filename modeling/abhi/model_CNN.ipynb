{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b4c7c3-78fb-4237-9d00-96393904004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base folder at /proj/dump/CNN_two_streams/run_2023_01_12, \n",
      "callbacks at /proj/dump/CNN_two_streams/run_2023_01_12/callback_data, \n",
      "predictions at /proj/dump/CNN_two_streams/run_2023_01_12/pred, \n",
      "model at /proj/dump/CNN_two_streams/run_2023_01_12/model, \n",
      "tmp at /proj/dump/CNN_two_streams/run_2023_01_12/tmp_data\n"
     ]
    }
   ],
   "source": [
    "from model_CNN_source import *\n",
    "dump_at = '/proj/dump/CNN_two_streams'\n",
    "source_data = '/proj/source_data/Training_Data'\n",
    "source_data_2 = '/proj/source_data/Testing_Data'\n",
    "processed_data = '/proj/processed_data'\n",
    "\n",
    "# run paths\n",
    "run_paths = set_dirs(dump_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5320754-7da4-4ecd-b8f5-54725ab6e28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59/2411887660.py:3: DtypeWarning: Columns (6,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  other_data = pd.read_csv(f\"{processed_data}/combined_mat_v2.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "geno_data = pd.read_csv(f\"{processed_data}/geno_processed.miss.1.mac.1.biallelic.txt\")\n",
    "other_data = pd.read_csv(f\"{processed_data}/combined_mat_v2.csv\")\n",
    "cv_data = read_json(f\"{processed_data}/train_test_split_v3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06ff888-55a6-4899-bdb4-39173b17f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify data sources\n",
    "all_cols = other_data.columns.values\n",
    "data_identifier_cols = [x for x in all_cols if \"Env\" in x][1:]\n",
    "data_identifier_cols_sl = [x for x in all_cols if \"sl_\" in x]\n",
    "data_identifier_cols_ec = [x for x in all_cols if \"ec_\" in x]\n",
    "data_identifier_cols_wt = [x for x in all_cols if \"wt_\" in x]\n",
    "data_identifier_bcols_non_pheno = data_identifier_cols_sl + data_identifier_cols_ec + data_identifier_cols_wt\n",
    "data_identifier_cols_non_pheno = [x for x in data_identifier_bcols_non_pheno if x not in data_identifier_cols] # these can be used as explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8abfac07-a37e-43bf-972e-32bfd7b510d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create response and predictor data\n",
    "\n",
    "## define useful columns\n",
    "useful_cols = ['Env', 'Loc', 'Year', 'Hybrid', 'Yield_Mg_ha'] + data_identifier_cols_non_pheno # in the test data we only get Env and Hybrid and need to predict yield_mg_ha\n",
    "pheno_data = other_data.loc[:, useful_cols] \n",
    "non_float_or_int = pd.DataFrame(pheno_data.dtypes[pheno_data.dtypes == \"object\"]).reset_index().loc[:, \"index\"].values.tolist()[3:]\n",
    "\n",
    "## remove columns which are string only\n",
    "useful_cols = [x for x in useful_cols if x not in non_float_or_int]\n",
    "\n",
    "## define a sample set for tuning hyperparameters\n",
    "test_index = np.sort(random.sample(pheno_data.index.tolist(), int(0.1*pheno_data.shape[0])))\n",
    "non_test = pheno_data.loc[~pheno_data.index.isin(test_index), :].index.tolist()\n",
    "val_index = np.sort(random.sample(non_test, int(0.1*len(non_test))))\n",
    "train_index = np.sort([x for x in non_test if x not in val_index])\n",
    "\n",
    "\n",
    "train = pheno_data.loc[train_index, useful_cols]\n",
    "train['type'] = 'train'\n",
    "val = pheno_data.loc[val_index, useful_cols]\n",
    "val['type'] = 'val'\n",
    "test = pheno_data.loc[test_index, useful_cols]\n",
    "test['type'] = 'test'\n",
    "phenoGE = pd.concat([train, val, test])\n",
    "\n",
    "## scale pheno data\n",
    "phenoGE_scaled, phenoGE_scaler = scale_data(phenoGE.loc[:, \"Yield_Mg_ha\"].values.reshape(-1, 1), [\"Yield_Mg_ha\"], phenoGE.index)\n",
    "phenoGE['y'] = phenoGE_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9853d1e-a4f2-4c08-8045-e8f343ebbfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn with two streams i.e. genetic and ec\n",
    "\n",
    "g_data_npy_path = f\"{run_paths['tmp_at']}/g_data.npy\"\n",
    "ec_data_npy_path = f\"{run_paths['tmp_at']}/ec_data.npy\"\n",
    "\n",
    "## Generate a array for genomic data\n",
    "if not Path(g_data_npy_path).is_file():\n",
    "    \n",
    "    geno_data_scaled, geno_data_scaler = scale_data(geno_data.iloc[:, 1:], geno_data.columns[1:], geno_data.index)\n",
    "    \n",
    "    geno_data_npy = []\n",
    "    \n",
    "    for hyb in phenoGE.Hybrid:\n",
    "        geno_data_npy.append(geno_data_scaled.loc[geno_data.Hybrid == hyb, :].values[0][1:].tolist())\n",
    "    \n",
    "    geno_data_npy_stacked = np.stack(geno_data_npy)\n",
    "    geno_data_npy_stacked_reshaped = geno_data_npy_stacked.reshape(geno_data_npy_stacked.shape[0], geno_data_npy_stacked.shape[1], 1) # of dimension = (samples, markers, 1) for one dimentional convulution\n",
    "    np.save(g_data_npy_path, geno_data_npy_stacked_reshaped)\n",
    "else:\n",
    "    geno_data_npy_stacked_reshaped = np.load(g_data_npy_path)\n",
    "\n",
    "## Generate a stream for ec data\n",
    "if not Path(ec_data_npy_path).is_file():\n",
    "\n",
    "    phenoGE_ec_scaled, phenoGE_ec_scaler = scale_data(phenoGE.loc[:, data_identifier_cols_ec[1:]], data_identifier_cols_ec[1:], phenoGE.index)\n",
    "    \n",
    "    ec_data_npy = []\n",
    "    \n",
    "    for env in phenoGE.Env:\n",
    "        ec_data_npy.append(phenoGE_ec_scaled.loc[phenoGE.Env == env, data_identifier_cols_ec[1:]].iloc[0, :].values.tolist())\n",
    "    \n",
    "    ec_data_npy_stacked = np.stack(ec_data_npy)\n",
    "    ec_data_npy_stacked_reshaped = ec_data_npy_stacked.reshape(ec_data_npy_stacked.shape[0], ec_data_npy_stacked.shape[1], 1) # of dimension = (samples, env_days, 1) for one dimentional convulution\n",
    "    \n",
    "    ## save for later pickup\n",
    "    np.save(ec_data_npy_path, ec_data_npy_stacked_reshaped)\n",
    "else:\n",
    "    ec_data_npy_stacked_reshaped = np.load(ec_data_npy_path)\n",
    "\n",
    "X = [geno_data_npy_stacked_reshaped, ec_data_npy_stacked_reshaped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8f642-a342-4b15-9d1e-879eb3ec741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data for model \n",
    "X_train, X_val, X_test = [geno_data_npy_stacked_reshaped[train_index], ec_data_npy_stacked_reshaped[train_index]], \\\n",
    "                         [geno_data_npy_stacked_reshaped[val_index], ec_data_npy_stacked_reshaped[val_index]], \\\n",
    "                         [geno_data_npy_stacked_reshaped[test_index], ec_data_npy_stacked_reshaped[test_index]]\n",
    "\n",
    "y_train, y_val, y_test = phenoGE.iloc[train_index, -1], \\\n",
    "                         phenoGE.iloc[val_index, -1], \\\n",
    "                         phenoGE.iloc[test_index, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e531636-5e34-421e-b392-e3f8ef286955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 161 Complete [01h 24m 09s]\n",
      "val_mean_squared_error: 0.008596484549343586\n",
      "\n",
      "Best val_mean_squared_error So Far: 0.008286920376121998\n",
      "Total elapsed time: 1d 20h 31m 48s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 11:41:50.087130: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) CNN_in_g, CNN_in_ec with unsupported characters which will be renamed to cnn_in_g, cnn_in_ec in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /proj/dump/CNN_two_streams/run_2023_01_04/model/best_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /proj/dump/CNN_two_streams/run_2023_01_04/model/best_model/assets\n"
     ]
    }
   ],
   "source": [
    "# perform tuning\n",
    "tuning_save_at = f\"{run_paths['model_at']}\"\n",
    "\n",
    "start_time_tuning = time.time()\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5, min_delta = 0.0001)\n",
    "\n",
    "tuner = kt.Hyperband(hypermodel=model_tuner_two_streams,\n",
    "                     objective=kt.Objective(\"val_mean_squared_error\", direction=\"min\"),\n",
    "                     max_epochs=100,\n",
    "                     factor=4,\n",
    "                     hyperband_iterations=1,\n",
    "                     overwrite = True,\n",
    "                     directory=tuning_save_at,\n",
    "                     project_name=\"hp_tuning\",\n",
    "                     seed=30)\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=100,\n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[stop_early],\n",
    "             batch_size = 64) #for non interactive - verbose - 0\n",
    "#top3_params = tuner.get_best_hyperparameters(num_trials=3)\n",
    "\n",
    "## generate model with best parameters\n",
    "best_model = tuner.get_best_models()[0]\n",
    "best_model.save(f\"{run_paths['model_at']}/best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15977db9-621a-4a0d-95e9-536913fc326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "best_model_loaded = tf.keras.models.load_model(f\"{run_paths['model_at']}/best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b738856-9056-4354-9c65-a94735a2bea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "598/598 [==============================] - 218s 364ms/step - loss: 0.0724 - mean_squared_error: 0.0088 - val_loss: 0.0667 - val_mean_squared_error: 0.0076\n",
      "Epoch 2/100\n",
      "598/598 [==============================] - 216s 362ms/step - loss: 0.0705 - mean_squared_error: 0.0084 - val_loss: 0.0657 - val_mean_squared_error: 0.0075\n",
      "Epoch 3/100\n",
      "598/598 [==============================] - 215s 359ms/step - loss: 0.0696 - mean_squared_error: 0.0082 - val_loss: 0.0662 - val_mean_squared_error: 0.0076\n",
      "Epoch 4/100\n",
      "598/598 [==============================] - 216s 361ms/step - loss: 0.0687 - mean_squared_error: 0.0080 - val_loss: 0.0656 - val_mean_squared_error: 0.0074\n",
      "Epoch 5/100\n",
      "598/598 [==============================] - 216s 361ms/step - loss: 0.0684 - mean_squared_error: 0.0080 - val_loss: 0.0653 - val_mean_squared_error: 0.0074\n",
      "Epoch 6/100\n",
      "598/598 [==============================] - 215s 359ms/step - loss: 0.0676 - mean_squared_error: 0.0079 - val_loss: 0.0657 - val_mean_squared_error: 0.0074\n",
      "Epoch 7/100\n",
      "598/598 [==============================] - 215s 359ms/step - loss: 0.0675 - mean_squared_error: 0.0078 - val_loss: 0.0655 - val_mean_squared_error: 0.0073\n",
      "Epoch 8/100\n",
      "598/598 [==============================] - 216s 362ms/step - loss: 0.0670 - mean_squared_error: 0.0077 - val_loss: 0.0651 - val_mean_squared_error: 0.0072\n",
      "Epoch 9/100\n",
      "598/598 [==============================] - 216s 361ms/step - loss: 0.0668 - mean_squared_error: 0.0077 - val_loss: 0.0649 - val_mean_squared_error: 0.0072\n",
      "Epoch 10/100\n",
      "598/598 [==============================] - 215s 359ms/step - loss: 0.0668 - mean_squared_error: 0.0077 - val_loss: 0.0650 - val_mean_squared_error: 0.0074\n",
      "Epoch 11/100\n",
      "598/598 [==============================] - 216s 361ms/step - loss: 0.0666 - mean_squared_error: 0.0076 - val_loss: 0.0642 - val_mean_squared_error: 0.0072\n",
      "Epoch 12/100\n",
      "598/598 [==============================] - 216s 361ms/step - loss: 0.0664 - mean_squared_error: 0.0076 - val_loss: 0.0633 - val_mean_squared_error: 0.0071\n",
      "Epoch 13/100\n",
      "598/598 [==============================] - 215s 359ms/step - loss: 0.0663 - mean_squared_error: 0.0076 - val_loss: 0.0636 - val_mean_squared_error: 0.0071\n",
      "Epoch 14/100\n",
      "598/598 [==============================] - 215s 359ms/step - loss: 0.0662 - mean_squared_error: 0.0075 - val_loss: 0.0636 - val_mean_squared_error: 0.0071\n",
      "Epoch 15/100\n",
      "598/598 [==============================] - 215s 359ms/step - loss: 0.0662 - mean_squared_error: 0.0075 - val_loss: 0.0646 - val_mean_squared_error: 0.0071\n",
      "Epoch 16/100\n",
      "598/598 [==============================] - 215s 359ms/step - loss: 0.0662 - mean_squared_error: 0.0075 - val_loss: 0.0638 - val_mean_squared_error: 0.0071\n",
      "Epoch 17/100\n",
      "598/598 [==============================] - 215s 359ms/step - loss: 0.0663 - mean_squared_error: 0.0076 - val_loss: 0.0653 - val_mean_squared_error: 0.0073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff810eea6d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "tb_filepath = run_paths['tb_cb']\n",
    "cp_filepath = run_paths['mc_cb']\n",
    "\n",
    "## set call backs\n",
    "tensorboard_cb = TensorBoard(tb_filepath)\n",
    "modelcheck_cb = ModelCheckpoint(filepath=cp_filepath,\n",
    "                                save_weights_only=True,\n",
    "                                monitor='val_loss',\n",
    "                                mode='min',\n",
    "                                save_best_only=True)\n",
    "model_cb = EarlyStopping(monitor='val_loss',\n",
    "                                 min_delta=0.00001,\n",
    "                                 patience=5,\n",
    "                                 verbose=0,\n",
    "                                 mode='min',\n",
    "                                 baseline=None,\n",
    "                                 restore_best_weights=True)\n",
    "best_model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                batch_size = 64,\n",
    "                epochs = 100,\n",
    "                verbose = 1,\n",
    "                shuffle = True,\n",
    "                callbacks=[modelcheck_cb, \n",
    "                           tensorboard_cb,\n",
    "                           model_cb])\n",
    "\n",
    "best_model.load_weights(cp_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9455c5-70dc-4195-9eb8-f88a388c982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform predictions\n",
    "prediction = best_model.predict(X_test)\n",
    "\n",
    "## re-scale data\n",
    "obs = inverse_scale(phenoGE_scaler, y_test, verbose = False)\n",
    "pred = inverse_scale(phenoGE_scaler, prediction, verbose = False)\n",
    "out_data = pd.DataFrame([test_index, obs, pred], index=[\"index\",\"Observed\",\"Predicted\"]).T\n",
    "out_data[\"index\"] = out_data[\"index\"].astype('int')\n",
    "\n",
    "## check rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
